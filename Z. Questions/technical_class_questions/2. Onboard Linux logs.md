📋 1️⃣ **Template: Questions to Ask the Client (Design Phase)**
Before touching Splunk configs, make sure to gather this info from the client:

✅ **General Requirements**
- What is the purpose of ingesting the logs? (e.g., monitoring, compliance, threat detection)
- Is there a specific log source type or OS version? (e.g., RHEL, Ubuntu, CentOS)
- Are there log volume estimates per day/per host?
- Are logs expected to grow over time?

🧱 **Architecture Questions**
- How many clusters or environments (e.g., dev, prod) are involved?
- Are clusters interconnected or isolated?
- Where are the Linux servers located? (On-prem, AWS, hybrid?)
- Will logs come through a Heavy Forwarder, Universal Forwarder, or syslog?

📦 **Index Requirements**
- Confirm index name preference (e.g., `linux_logs_abc`)
- Confirm index size limit: 4 GB total? Or 4 GB/day?
- Confirm retention policy: 4 months
- Any indexer clustering involved? (affects index creation and replication)

🔐 **Security & Access**
- Are logs sensitive? (PII, payment info, etc.)
- Do logs need to be filtered or masked before indexing?
- Who needs access to search the logs?

🔄 **Operational Details**
- How many Linux servers are we onboarding?
- Is there a standard logging format? (e.g., syslog, journald, custom scripts)
- Should we use `inputs.conf` on Universal Forwarders or central log collector?
- Are there any specific fields or tags required for parsing?

---

⚙️ 2️⃣ **Steps to Ingest Linux Logs into Splunk (With Limits)**
🎯 *Objective Recap:*
Onboard Linux logs from multiple clusters into Splunk. Use a new index. Storage limit = 4GB (assumed daily). Retention = 4 months.

✅ **Step-by-Step Implementation**

**Step 1: Create a New Index**
On the indexer(s) or via the deployer in a clustered setup:

```ini
# indexes.conf
[linux_logs_abc]
homePath   = $SPLUNK_DB/linux_logs_abc/db
coldPath   = $SPLUNK_DB/linux_logs_abc/colddb
thawedPath = $SPLUNK_DB/linux_logs_abc/thaweddb
maxTotalDataSizeMB = 5000           # ~5GB total limit
frozenTimePeriodInSecs = 10368000   # 4 months = 120 days = 10368000 secs
```

🔄 Deploy this config if you're using a cluster, or define it locally if standalone.

**Step 2: Configure Log Forwarding on Linux Servers**
If using Universal Forwarders, configure inputs:

```ini
# inputs.conf on the UF (or Heavy Forwarder if central)
[monitor:///var/log]
disabled = false
index = linux_logs_abc
sourcetype = linux_syslog
recursive = true
_blacklist = \.gz$  # Optional: avoid archived logs
```

And define output (if not already set):

```ini
# outputs.conf
[tcpout:primary-indexers]
server = indexer1.abc.com:9997,indexer2.abc.com:9997
```

**Step 3: (Optional) Set Props & Transforms**
To format logs, tag sources, or filter out unnecessary data:

```ini
# props.conf
[linux_syslog]
TRANSFORMS-null = setnull

# transforms.conf
[setnull]
REGEX = unnecessary_line_pattern
DEST_KEY = queue
FORMAT = nullQueue
```

**Step 4: Validate & Monitor Ingestion**
- Use `index=linux_logs_abc` in Splunk Search Head
- Check if log volume is within the 4GB/day limit
- Monitor with the Monitoring Console (DMC)
- Set alerts for over-indexing or dropped logs

✅ **Optional: Throttle or Limit Data**
- Use `props.conf` + `transforms.conf` to discard verbose events
- Use `limits.conf` to throttle indexing per sourcetype
- Enable data sampling (only for non-critical logs)

---

🔁 **Time-Based Retention vs 📦 Volume-Based Retention**

| Feature | ⏳ Time-Based Retention | 📦 Volume-Based Retention |
|--------|------------------------|--------------------------|
| **Definition** | Logs are retained for a specific period of time (e.g., 4 months, 365 days). | Logs are retained until a certain volume threshold is reached (e.g., 100 GB, 4 TB). |
| **Trigger to Delete Old Logs** | Based on the age of the data (e.g., delete logs older than 120 days). | Based on storage size used — oldest data is deleted when the limit is hit. |
| **Example** | "Retain logs for 90 days regardless of size." | "Retain up to 500 GB of logs; once full, delete oldest logs first." |
| **Use Case** | Common in compliance-driven environments (HIPAA, PCI-DSS). | Used in environments where storage cost or limits are a concern. |
| **Splunk Configuration** | Controlled via `frozenTimePeriodInSecs` in `indexes.conf`. | Controlled via `maxTotalDataSizeMB` in `indexes.conf`. |
| **Priority** | Keeps data as long as the time window allows. | Keeps data as long as space is available, even if it's newer. |

🧠 **Which One Takes Precedence in Splunk?**
In Splunk:

> **Both** can be set for an index.
>
> Data is deleted (rolled to frozen) when **either limit is hit**.

Whichever comes first:
- 🔹 The data reaches its time limit (`frozenTimePeriodInSecs`)
- 🔹 The index reaches its size cap (`maxTotalDataSizeMB`)

🔐 **Real-World Example:**
Let’s say:
- Your index has `frozenTimePeriodInSecs = 10368000` (120 days)
- And `maxTotalDataSizeMB = 40960` (40 GB)

**Scenario A:** You generate only 100 MB/day. You’ll hit 120 days first → time-based retention applies.

**Scenario B:** You generate 2 GB/day. You’ll hit 40 GB before 120 days → volume-based retention kicks in first.

